{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '500M_multi_species_v2'\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    max_positions=5800,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_MAPPING = {\n",
    "  'A': ['GCT', 'GCC', 'GCA', 'GCG'],\n",
    "  'R': ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n",
    "  'N': ['AAT', 'AAC'],\n",
    "  'D': ['GAT', 'GAC'],\n",
    "  'C': ['TGT', 'TGC'],\n",
    "  'Q': ['CAA', 'CAG'],\n",
    "  'E': ['GAA', 'GAG'],\n",
    "  'G': ['GGT', 'GGC', 'GGA', 'GGG'],\n",
    "  'H': ['CAT', 'CAC'],\n",
    "  'I': ['ATT', 'ATC', 'ATA'],\n",
    "  'L': ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n",
    "  'K': ['AAA', 'AAG'],\n",
    "  'M': ['ATG'],\n",
    "  'F': ['TTT', 'TTC'],\n",
    "  'P': ['CCT', 'CCC', 'CCA', 'CCG'],\n",
    "  'O': ['TAG'],\n",
    "  'U': ['TGA'],\n",
    "  'S': ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n",
    "  'T': ['ACT', 'ACC', 'ACA', 'ACG'],\n",
    "  'W': ['TGG'],\n",
    "  'Y': ['TAT', 'TAC'],\n",
    "  'V': ['GTT', 'GTC', 'GTA', 'GTG'],\n",
    "}\n",
    "\n",
    "def most_frequent_codons(protein: str) -> str:\n",
    "    # source: https://www.genscript.com/tools/codon-frequency-table\n",
    "    # Manually took the highest frequency codon for each AA\n",
    "    CANONICAL_CODON = {\n",
    "        'F': 'TTT',\n",
    "        'Y': 'TAT',\n",
    "        'L': 'CTG',\n",
    "        'H': 'CAT',\n",
    "        'Q': 'CAG',\n",
    "        'I': 'ATT',\n",
    "        'M': 'ATG',\n",
    "        'N': 'AAC',\n",
    "        'K': 'AAA',\n",
    "        'V': 'GTG',\n",
    "        'D': 'GAT',\n",
    "        'E': 'GAA',\n",
    "        'S': 'AGC',\n",
    "        'C': 'TGC',\n",
    "        'P': 'CCG',\n",
    "        'R': 'CGT',\n",
    "        'T': 'ACC',\n",
    "        'A': 'GCG',\n",
    "        'G': 'GGC',\n",
    "        'W': 'TGG',\n",
    "    }\n",
    "    return \"\".join(CANONICAL_CODON[x] for x in protein)\n",
    "\n",
    "def switch_codon(protein: str) -> str:\n",
    "    basis = most_frequent_codons(protein)\n",
    "    position = random.choice(range(len(protein))) \n",
    "    res = basis[:(position)*3] + random.choice(AA_MAPPING[protein[position]]) + basis[(position+1)*3:]\n",
    "    return ''.join(res)\n",
    "\n",
    "def generate_plasmids(protein: str, k: int):\n",
    "    res = set([most_frequent_codons(protein)]) \n",
    "    while True:\n",
    "        candidate = switch_codon(protein)\n",
    "        res.add(candidate)\n",
    "        if len(res) >= k:\n",
    "            break           \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/icodon/training.csv.gz', index_col=0, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.loc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = train_df['coding'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Inputs to the learned positional embeddings layer have a length 5000 greater than the max positions used to instantiate it: 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/haiku/_src/transform.py:187\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    181\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    182\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mIf the functions you are transforming use the same names you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m out, state \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mapply(params, \u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m state:\n\u001b[1;32m    189\u001b[0m   \u001b[39mraise\u001b[39;00m base\u001b[39m.\u001b[39mNonEmptyStateError(\n\u001b[1;32m    190\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mIf your transformed function uses `hk.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mget,set}_state` then use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`hk.transform_with_state`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/haiku/_src/transform.py:457\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mwith\u001b[39;00m base\u001b[39m.\u001b[39mnew_context(params\u001b[39m=\u001b[39mparams, state\u001b[39m=\u001b[39mstate, rng\u001b[39m=\u001b[39mrng) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    456\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m     out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    458\u001b[0m   \u001b[39mexcept\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    459\u001b[0m     \u001b[39mraise\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/nucleotide_transformer/model.py:392\u001b[0m, in \u001b[0;36mbuild_nucleotide_transformer_fn.<locals>.nucleotide_transformer_fn\u001b[0;34m(tokens, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39m# Run the encoder over the inputs.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m encoder \u001b[39m=\u001b[39m NucleotideTransformer(config\u001b[39m=\u001b[39mmodel_config, name\u001b[39m=\u001b[39mmodel_name)\n\u001b[0;32m--> 392\u001b[0m outs \u001b[39m=\u001b[39m encoder(\n\u001b[1;32m    393\u001b[0m     tokens\u001b[39m=\u001b[39;49mtokens,\n\u001b[1;32m    394\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    395\u001b[0m )\n\u001b[1;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/haiku/_src/module.py:465\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[39mif\u001b[39;00m method_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    463\u001b[0m     f \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnamed_call(f, name\u001b[39m=\u001b[39mmethod_name)\n\u001b[0;32m--> 465\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    467\u001b[0m \u001b[39m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/haiku/_src/module.py:323\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 323\u001b[0m \u001b[39mreturn\u001b[39;00m next_fun(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/haiku/_src/module.py:319\u001b[0m, in \u001b[0;36mrun_interceptors.<locals>.next_fun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m interceptor_stack_copy:\n\u001b[1;32m    316\u001b[0m   \u001b[39m# NOTE: The `interceptor_fun` may call `next_fun` to trigger the next\u001b[39;00m\n\u001b[1;32m    317\u001b[0m   \u001b[39m# interceptor (and so on) allowing interceptors to be run in turn.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m   interceptor_fun \u001b[39m=\u001b[39m interceptor_stack_copy\u001b[39m.\u001b[39mpopleft()\n\u001b[0;32m--> 319\u001b[0m   \u001b[39mreturn\u001b[39;00m interceptor_fun(next_fun, args, kwargs, ctx)\n\u001b[1;32m    320\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/haiku/_src/mixed_precision.py:286\u001b[0m, in \u001b[0;36m_mixed_precision_interceptor\u001b[0;34m(next_f, args, kwargs, context)\u001b[0m\n\u001b[1;32m    283\u001b[0m ctx\u001b[39m.\u001b[39menter_context(_thread_local_state\u001b[39m.\u001b[39mpush_current_policy(policy))\n\u001b[1;32m    285\u001b[0m args, kwargs \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mcast_to_compute((args, kwargs))\n\u001b[0;32m--> 286\u001b[0m out \u001b[39m=\u001b[39m next_f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m policy\u001b[39m.\u001b[39mcast_to_output(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/haiku/_src/module.py:321\u001b[0m, in \u001b[0;36mrun_interceptors.<locals>.next_fun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m   \u001b[39mreturn\u001b[39;00m interceptor_fun(next_fun, args, kwargs, ctx)\n\u001b[1;32m    320\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nucleo/lib/python3.11/site-packages/nucleotide_transformer/model.py:311\u001b[0m, in \u001b[0;36mNucleotideTransformer.__call__\u001b[0;34m(self, tokens, attention_mask)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mpositional_embedding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlearned\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39m# Add check that the sequence fed into the transformer is not longer\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39m# than the max positions used to instantiate the learned positional\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[39m# embeddings layer\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     max_length_authorized \u001b[39m=\u001b[39m (\n\u001b[1;32m    307\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos_embed_layer\u001b[39m.\u001b[39m_embed_layer\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    308\u001b[0m         \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos_embed_layer\u001b[39m.\u001b[39mpadding_idx\n\u001b[1;32m    309\u001b[0m         \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m     \u001b[39massert\u001b[39;00m tokens\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m max_length_authorized, (\n\u001b[1;32m    312\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInputs to the learned positional embeddings layer have a length \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m greater than the max positions used to instantiate \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mit: \u001b[39m\u001b[39m{\u001b[39;00mmax_length_authorized\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m     )\n\u001b[1;32m    316\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos_embed_layer(tokens)\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39memb_layer_norm_before:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Inputs to the learned positional embeddings layer have a length 5000 greater than the max positions used to instantiate it: 1000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize random key\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Infer\n",
    "outs = forward_fn.apply(parameters, random_key, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(tokens, model, random_key, batch_size=32):\n",
    "    forward_fn, parameters = model\n",
    "    for i in range(0, len(tokens), batch_size):\n",
    "        batch = tokens[i:min(i+batch_size, len(tokens))]\n",
    "        yield forward_fn.apply(parameters, random_key, batch)['embeddings_20']\n",
    "        \n",
    "def get_embeddings(tokens, model, random_key, batch_size=32):\n",
    "    forward_fn, parameters = model\n",
    "    res = []\n",
    "    for i in range(0, len(tokens), batch_size):\n",
    "        batch = tokens[i:min(i+batch_size, len(tokens))]\n",
    "        res.append(forward_fn.apply(parameters, random_key, batch)['embeddings_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool(embeddings, lengths):\n",
    "    res = np.zeros((embeddings.shape[0], embeddings.shape[2]))\n",
    "    for i in range(embeddings.shape[0]):\n",
    "        res[i, :] = jnp.mean(embeddings[i,0:lengths[i],:], axis=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    return X - np.mean(X, axis=0) / np.std(X, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nucleo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
